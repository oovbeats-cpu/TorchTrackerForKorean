#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
tlidb.com ìŠ¤í‚¬/ì‹ ê²© ì¹´í…Œê³ ë¦¬ í¬ë¡¤ëŸ¬ (Crawler #4)

í¬ë¡¤ë§ ëŒ€ìƒ:
- /ko/Fluorescent_Memory (ì°¬ë€í•œ ê¸°ì–µ)
- /ko/Active_Skill (ì•¡í‹°ë¸Œ ìŠ¤í‚¬)
- /ko/Support_Skill (ë³´ì¡° ìŠ¤í‚¬)
- /ko/Divinity_Emblems (ì‹ ë ¥ ì— ë¸”ëŸ¼)
"""

import json
import time
import re
import sys
from typing import Dict, List, Optional
from pathlib import Path
import sys
import codecs

# Windows ì½˜ì†” UTF-8 ê°•ì œ ì„¤ì • (í•œê¸€ ê¹¨ì§ ë°©ì§€)
if sys.platform == 'win32':
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')



# Windows ì½˜ì†” UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    print("[ERROR] í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”: pip install requests beautifulsoup4 lxml")
    exit(1)


# ====================
# ì„¤ì •
# ====================
BASE_URL = "https://tlidb.com"
USER_AGENT = "TITrack/1.0.2 (+https://github.com/yourusername/TorchTrackerForKorean)"
RATE_LIMIT = 0.5  # ì´ˆë‹¹ 2 ìš”ì²­ (ë” ë¹ ë¥´ê²Œ)
OUTPUT_DIR = Path(__file__).parent.parent / "output"
OUTPUT_FILE = OUTPUT_DIR / "crawler4_skills.json"
FETCH_DETAILS = False  # ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ìƒëµ (ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ì— ëª¨ë“  ì •ë³´ ìˆìŒ)

# í¬ë¡¤ë§ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬
CATEGORIES = [
    {"path": "/ko/Fluorescent_Memory", "name": "ì°¬ë€í•œ ê¸°ì–µ", "type": "ê¸°ì–µ"},
    {"path": "/ko/Active_Skill", "name": "ì•¡í‹°ë¸Œ ìŠ¤í‚¬", "type": "ìŠ¤í‚¬"},
    {"path": "/ko/Support_Skill", "name": "ë³´ì¡° ìŠ¤í‚¬", "type": "ìŠ¤í‚¬"},
    {"path": "/ko/Divinity_Emblems", "name": "ì‹ ë ¥ ì— ë¸”ëŸ¼", "type": "ì— ë¸”ëŸ¼"},
]

# í†µê³„
stats = {
    "categories_crawled": 0,
    "items_found": 0,
    "items_extracted": 0,
    "errors": 0,
    "requests_made": 0,
}


# ====================
# HTTP ìœ í‹¸ë¦¬í‹°
# ====================
def make_request(url: str, max_retries: int = 3) -> Optional[requests.Response]:
    """Rate limit ì¤€ìˆ˜ HTTP ìš”ì²­"""
    stats["requests_made"] += 1

    for attempt in range(max_retries):
        try:
            time.sleep(RATE_LIMIT)  # Rate limiting
            response = requests.get(
                url,
                headers={"User-Agent": USER_AGENT},
                timeout=(10, 30),  # (connect, read)
            )
            response.raise_for_status()
            return response
        except requests.RequestException as e:
            wait_time = (2 ** attempt) * RATE_LIMIT  # Exponential backoff
            print(f"[WARNING]  ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                print(f"   {wait_time:.1f}ì´ˆ í›„ ì¬ì‹œë„...")
                time.sleep(wait_time)
            else:
                stats["errors"] += 1
                return None
    return None


# ====================
# íŒŒì‹± í•¨ìˆ˜
# ====================
def extract_config_base_id(url_or_text: str) -> Optional[int]:
    """URL ë˜ëŠ” í…ìŠ¤íŠ¸ì—ì„œ ConfigBaseId ì¶”ì¶œ"""
    # íŒ¨í„´: /item/123456 ë˜ëŠ” data-id="123456" ë“±
    match = re.search(r'/item/(\d+)', url_or_text)
    if match:
        return int(match.group(1))

    match = re.search(r'data-id["\s]*=\s*["\s]*(\d+)', url_or_text)
    if match:
        return int(match.group(1))

    match = re.search(r'\b(\d{6,})\b', url_or_text)  # 6ìë¦¬ ì´ìƒ ìˆ«ì
    if match:
        return int(match.group(1))

    return None


def parse_category_page(html: str, category_type: str) -> List[Dict]:
    """ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ì—ì„œ ì•„ì´í…œ ëª©ë¡ ì¶”ì¶œ"""
    soup = BeautifulSoup(html, 'lxml')
    items = []

    # tlidb.com íŠ¹í™” íŒŒì‹±: div.row.row-cols-* êµ¬ì¡°
    # <div class="col"><div class="d-flex border-top rounded">
    #   <a data-hover="?s=ItemBase%2F6002" href="...">ì•„ì´í…œëª…</a>

    # íŒ¨í„´ 1: tlidb.comì˜ ê·¸ë¦¬ë“œ ë ˆì´ì•„ì›ƒ (ê°€ì¥ ìš°ì„ )
    grid_items = soup.select('div.row div.col div.d-flex')
    for item_div in grid_items:
        link = item_div.find('a', attrs={'data-hover': True})
        if not link:
            continue

        # data-hover="?s=ItemBase%2F6002" í˜•ì‹ì—ì„œ ID ì¶”ì¶œ (%2FëŠ” URL ì¸ì½”ë”©ëœ /)
        hover_attr = link.get('data-hover', '')
        config_id = None

        # ItemBase/ID ë˜ëŠ” ItemBase%2FID í˜•ì‹ ì°¾ê¸°
        match = re.search(r'ItemBase(?:%2F|/)(\d+)', hover_attr)
        if match:
            config_id = int(match.group(1))
        else:
            # hrefì—ì„œë„ ì‹œë„
            href = link.get('href', '')
            match = re.search(r'/item/(\d+)', href)
            if match:
                config_id = int(match.group(1))

        if not config_id:
            continue

        # í•œêµ­ì–´ ì´ë¦„ ì¶”ì¶œ
        name_ko = link.get_text(strip=True)

        if name_ko and config_id:
            items.append({
                "config_base_id": config_id,
                "name_ko": name_ko,
                "type": category_type,
                "url": f"{BASE_URL}/{link['href']}" if not link['href'].startswith('http') else link['href']
            })

    # íŒ¨í„´ 2: í…Œì´ë¸” í˜•ì‹ (í´ë°±)
    if not items:
        rows = soup.select('table.item-table tr, table.items-table tr, table tbody tr')
        for row in rows:
            link = row.find('a', href=re.compile(r'/item/\d+'))
            if not link:
                continue

            config_id = extract_config_base_id(link['href'])
            if not config_id:
                continue

            name_ko = (
                link.get('title', '').strip() or
                link.get_text(strip=True) or
                link.get('data-name', '').strip()
            )

            if name_ko and config_id:
                items.append({
                    "config_base_id": config_id,
                    "name_ko": name_ko,
                    "type": category_type,
                    "url": f"{BASE_URL}{link['href']}"
                })

    # íŒ¨í„´ 3: ì¹´ë“œ í˜•ì‹ (í´ë°±)
    if not items:
        cards = soup.select('div.item-card, div.skill-card, a.item-link')
        for card in cards:
            link = card if card.name == 'a' else card.find('a', href=re.compile(r'/item/\d+'))
            if not link:
                continue

            config_id = extract_config_base_id(link.get('href', ''))
            if not config_id:
                continue

            name_elem = card.select_one('.item-name, .skill-name, h3, h4')
            name_ko = name_elem.get_text(strip=True) if name_elem else link.get_text(strip=True)

            if name_ko and config_id:
                items.append({
                    "config_base_id": config_id,
                    "name_ko": name_ko,
                    "type": category_type,
                    "url": f"{BASE_URL}{link['href']}"
                })

    return items


def extract_item_details(html: str, item_data: Dict) -> Dict:
    """ê°œë³„ ì•„ì´í…œ í˜ì´ì§€ì—ì„œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
    soup = BeautifulSoup(html, 'lxml')

    # í•œêµ­ì–´ ì´ë¦„ ì¬í™•ì¸ (ìƒì„¸ í˜ì´ì§€ê°€ ë” ì •í™•í•  ìˆ˜ ìˆìŒ)
    name_elem = soup.select_one('h1.item-name, h1, .page-title')
    if name_elem:
        name_text = name_elem.get_text(strip=True)
        if name_text and len(name_text) > len(item_data.get("name_ko", "")):
            item_data["name_ko"] = name_text

    # ì•„ì´ì½˜ URL ì¶”ì¶œ
    icon_elem = soup.select_one('img.item-icon, img[src*="icon"], img[src*="skill"]')
    if icon_elem and icon_elem.get('src'):
        icon_url = icon_elem['src']
        if not icon_url.startswith('http'):
            icon_url = BASE_URL + icon_url
        item_data["icon"] = icon_url

    # ì„¤ëª… ì¶”ì¶œ (ì„ íƒ ì‚¬í•­)
    desc_elem = soup.select_one('.item-description, .skill-description, .description')
    if desc_elem:
        item_data["description"] = desc_elem.get_text(strip=True)[:200]  # ìµœëŒ€ 200ì

    return item_data


# ====================
# ë©”ì¸ í¬ë¡¤ë§ ë¡œì§
# ====================
def crawl_category(category: Dict) -> List[Dict]:
    """ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ í¬ë¡¤ë§"""
    print(f"\n{'='*60}")
    print(f"[DIR] ì¹´í…Œê³ ë¦¬: {category['name']} ({category['path']})")
    print(f"{'='*60}")

    url = BASE_URL + category['path']
    response = make_request(url)

    if not response:
        print(f"[ERROR] ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {url}")
        return []

    items = parse_category_page(response.text, category['type'])
    print(f"[OK] {len(items)}ê°œ ì•„ì´í…œ ë°œê²¬")

    stats["categories_crawled"] += 1
    stats["items_found"] += len(items)

    # ê°œë³„ ì•„ì´í…œ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ (ì„ íƒì )
    extracted_items = []
    for i, item in enumerate(items, 1):
        print(f"   [{i}/{len(items)}] {item['name_ko']} (ID: {item['config_base_id']})")

        if FETCH_DETAILS:
            detail_response = make_request(item['url'])
            if detail_response:
                item = extract_item_details(detail_response.text, item)
                stats["items_extracted"] += 1
        else:
            stats["items_extracted"] += 1

        # URL ì œê±° (ì¶œë ¥ íŒŒì¼ì— ë¶ˆí•„ìš”)
        item.pop("url", None)
        extracted_items.append(item)

    return extracted_items


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 80)
    print("[SEARCH] TITrack Crawler #4: ìŠ¤í‚¬/ì‹ ê²© ì¹´í…Œê³ ë¦¬")
    print("=" * 80)
    print(f"ëŒ€ìƒ ì‚¬ì´íŠ¸: {BASE_URL}")
    print(f"ì¶œë ¥ íŒŒì¼: {OUTPUT_FILE}")
    print(f"Rate Limit: {RATE_LIMIT}ì´ˆ/ìš”ì²­")

    OUTPUT_DIR.mkdir(exist_ok=True)

    all_items = []

    # ê° ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
    for category in CATEGORIES:
        items = crawl_category(category)
        all_items.extend(items)

    # TITrack items_ko.json í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    items_dict = {}
    for item in all_items:
        config_id = str(item["config_base_id"])
        items_dict[config_id] = {
            "name": item["name_ko"],
            "type": item["type"],
            "price": 0  # ìŠ¤í‚¬/ì‹ ê²©ì€ ê±°ë˜ ë¶ˆê°€
        }

        # ì•„ì´ì½˜ URLì´ ìˆìœ¼ë©´ ì¶”ê°€ (ì„ íƒ ì‚¬í•­)
        if "icon" in item:
            items_dict[config_id]["icon"] = item["icon"]

    # JSON ì €ì¥
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(items_dict, f, ensure_ascii=False, indent=2)

    # í†µê³„ ì¶œë ¥
    print("\n" + "=" * 80)
    print("[STATS] í¬ë¡¤ë§ ì™„ë£Œ í†µê³„")
    print("=" * 80)
    print(f"[OK] í¬ë¡¤ë§í•œ ì¹´í…Œê³ ë¦¬: {stats['categories_crawled']}/{len(CATEGORIES)}")
    print(f"[OK] ë°œê²¬í•œ ì•„ì´í…œ: {stats['items_found']}ê°œ")
    print(f"[OK] ì¶”ì¶œí•œ ì•„ì´í…œ: {stats['items_extracted']}ê°œ")
    print(f"[ERROR] ì˜¤ë¥˜ ë°œìƒ: {stats['errors']}íšŒ")
    print(f"ğŸ“¡ ì´ ìš”ì²­ ìˆ˜: {stats['requests_made']}íšŒ")
    print(f"ğŸ’¾ ì¶œë ¥ íŒŒì¼: {OUTPUT_FILE}")
    print(f"ğŸ“¦ ìµœì¢… ì•„ì´í…œ ìˆ˜: {len(items_dict)}ê°œ")

    if stats['errors'] > 0:
        print(f"\n[WARNING]  {stats['errors']}ê°œ ì˜¤ë¥˜ ë°œìƒ - ë¡œê·¸ í™•ì¸ í•„ìš”")

    print("\n[OK] í¬ë¡¤ë§ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
